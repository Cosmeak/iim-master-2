{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Transformer Model from scratch\n",
    "\n",
    "The Transformer model is a groundbreaking architecture in AI, especially for Natural Language Processing (NLP) tasks like language translation, text generation, and more. It was introduced in 2017 by Vaswani et al. in a paper titled \"Attention Is All You Need\". This model was designed to handle sequential data (like sentences) in a much more efficient way than previous models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks.\n",
    "\n",
    "## Key Components of the Transformer\n",
    "\n",
    "The transformer architecture has a few critical building blocks that set it apart:\n",
    "\n",
    "### a. Self-Attention Mechanism\n",
    "\n",
    "What is Attention? The self-attention mechanism helps the model figure out which words in a sentence are most important relative to other words. For example, in the sentence \"The cat sat on the mat.\", the model needs to understand that \"sat\" relates closely to both \"cat\" and \"mat.\"\n",
    "\n",
    "Why Self-Attention? Traditional RNNs processed words one-by-one (sequentially), which makes it harder to understand long-range dependencies. With self-attention, the model can look at all words in the sentence at the same time and weigh their relationships.\n",
    "\n",
    "How Does It Work? Self-attention takes in three components for each word:\n",
    "- Query (Q)\n",
    "- Key (K)\n",
    "- Value (V)\n",
    "\n",
    "Each word in a sentence will compute attention scores by comparing its Query to all other words' Keys. These attention scores tell us how much focus should be placed on each word when generating a word’s output. The result is weighted by the Value of the word.\n",
    "\n",
    "### b. Multi-Head Attention\n",
    "\n",
    "Instead of having just one attention mechanism, the transformer model uses multiple attention heads. Each head learns a different aspect of the relationships between words in the sentence.\n",
    "\n",
    "Why Multiple Heads? With multiple heads, the model can capture different patterns in the relationships. For instance, one head may focus on subject-verb relationships, while another might focus on noun-adjective pairings.\n",
    "\n",
    "### c. Positional Encoding\n",
    "\n",
    "Transformers process input data all at once (in parallel), so they don't inherently know the order of the words in a sentence.\n",
    "\n",
    "To fix this, positional encoding is added to the input embeddings to represent the position of each word in the sequence. This encoding ensures the model understands the order of the words, which is crucial for meaning.\n",
    "\n",
    "### d. Feed-Forward Neural Networks\n",
    "\n",
    "After the attention layers, the transformer uses feed-forward layers (fully connected layers) to process the output from the attention mechanism and transform the data further.\n",
    "\n",
    "These layers help learn complex transformations that improve the overall model’s performance.\n",
    "\n",
    "## Why Transformers Are So Effective\n",
    "### a. Parallelization\n",
    "\n",
    "One of the key advantages of transformers is that they process all the words in a sentence simultaneously (in parallel). Unlike RNNs or LSTMs, which process one word at a time, transformers don’t have to wait for the previous word’s processing to be completed before moving on to the next one.\n",
    "This makes transformers much faster to train and scalable to large datasets.\n",
    "\n",
    "### b. Long-Range Dependencies\n",
    "\n",
    "Transformers are better at capturing long-range dependencies in text. For instance, in a sentence like “The man who was standing at the door left the house,” the model can connect “man” with “left” even though there are words in between. Traditional models like RNNs struggle with such dependencies, especially when sentences are long.\n",
    "\n",
    "### c. Flexibility in Tasks\n",
    "\n",
    "Transformers can handle a variety of tasks like translation, summarization, question-answering, and text generation.\n",
    "This flexibility is because the encoder-decoder architecture can be adapted. For instance, the decoder part can be used for generating text, while the encoder can be used for understanding and classifying text.\n",
    "\n",
    "## How Transformers are Used in Real Life\n",
    "\n",
    "Transformers have revolutionized the field of AI, and they're the backbone of many cutting-edge models:\n",
    "\n",
    "- GPT (Generative Pre-trained Transformer): This is the model behind AI tools like ChatGPT. It uses a transformer decoder architecture to generate human-like text.\n",
    "- BERT (Bidirectional Encoder Representations from Transformers): BERT uses the transformer encoder to understand context and perform tasks like sentiment analysis, named entity recognition, and more.\n",
    "- T5 (Text-to-Text Transfer Transformer): T5 treats every NLP problem as a text-to-text problem. For example, translating text from one language to another is treated as \"Translate English to French: [text].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq = seq\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create a matrix of shape (seq, d_model)\n",
    "        pe = torch.zeros(seq, d_model)\n",
    "        \n",
    "        # Create a vector of shape (seq)\n",
    "        position = torch.arange(0, seq, dtype=torch.float).unsqueeze(1) # (seq, 1)\n",
    "        \n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        \n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        \n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        \n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq, d_model)\n",
    "        \n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha (multiplicative) is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias (additive) is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq, hidden_size)\n",
    "        # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq, d_model) --> (batch, seq, d_ff) --> (batch, seq, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq, d_k) --> (batch, h, seq, seq)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq, seq) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq, seq) --> (batch, h, seq, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq, d_model) --> (batch, seq, d_model)\n",
    "        key = self.w_k(k) # (batch, seq, d_model) --> (batch, seq, d_model)\n",
    "        value = self.w_v(v) # (batch, seq, d_model) --> (batch, seq, d_model)\n",
    "\n",
    "        # (batch, seq, d_model) --> (batch, seq, h, d_k) --> (batch, h, seq, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq, d_k) --> (batch, seq, h, d_k) --> (batch, seq, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq, d_model) --> (batch, seq, d_model)  \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "    \n",
    "        def forward(self, x, sublayer):\n",
    "            return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture: Encoder-Decoder Structure\n",
    "\n",
    "The transformer model is typically divided into two parts:\n",
    "\n",
    "- Encoder: This part processes the input data (e.g., a sentence in one language).\n",
    "- Decoder: This part generates the output data (e.g., the translation of that sentence).\n",
    "\n",
    "Each of these parts is made up of layers that are stacked on top of each other:\n",
    "- Encoder Layers: Each encoder layer consists of two main components:\n",
    "    - Multi-head self-attention\n",
    "    - Feed-forward neural network\n",
    "- Decoder Layers: The decoder is similar, but with an additional layer of masked multi-head self-attention (which ensures that the decoder cannot \"cheat\" by looking ahead at future words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        features: int, \n",
    "        self_attention_block: MultiHeadAttentionBlock, \n",
    "        cross_attention_block: MultiHeadAttentionBlock, \n",
    "        feed_forward_block: FeedForwardBlock, \n",
    "        dropout: float\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq, d_model) --> (batch, seq, vocab_size)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder: Encoder, \n",
    "        decoder: Decoder, \n",
    "        src_embed: InputEmbeddings, \n",
    "        tgt_embed: InputEmbeddings, \n",
    "        src_pos: PositionalEncoding, \n",
    "        tgt_pos: PositionalEncoding, \n",
    "        projection_layer: ProjectionLayer\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq, vocab_size)\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(\n",
    "    src_vocab_size: int, \n",
    "    tgt_vocab_size: int, \n",
    "    src_seq: int, \n",
    "    tgt_seq: int, \n",
    "    d_model: int=512, \n",
    "    N: int=6, \n",
    "    h: int=8, \n",
    "    dropout: float=0.1, \n",
    "    d_ff: int=2048\n",
    ") -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[SOS] → Signals the start of a sentence (important for decoding).\n",
    "[EOS] → Indicates the end of a sentence (used in labels).\n",
    "[PAD] → Pads shorter sentences to match seq_len (ignored during computation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, num_merges=100):\n",
    "        self.num_merges = num_merges\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "\n",
    "        # Special tokens\n",
    "        self.special_tokens = {\n",
    "            \"[SOS]\": 0,\n",
    "            \"[EOS]\": 1,\n",
    "            \"[PAD]\": 2\n",
    "        }\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer on a given text corpus.\n",
    "        \"\"\"\n",
    "        word_freqs = {}\n",
    "        for sentence in corpus:\n",
    "            words = sentence.split()\n",
    "            for word in words:\n",
    "                word_freqs[word] = word_freqs.get(word, 0) + 1\n",
    "\n",
    "        # Initialize vocabulary with characters\n",
    "        self.vocab = {char: idx + 3 for idx, char in enumerate(set(\"\".join(word_freqs.keys())))}\n",
    "        self.vocab.update(self.special_tokens)\n",
    "\n",
    "        # Merge operations\n",
    "        for _ in range(self.num_merges):\n",
    "            pairs = {}\n",
    "            for word, freq in word_freqs.items():\n",
    "                symbols = list(word)\n",
    "                for i in range(len(symbols) - 1):\n",
    "                    pair = (symbols[i], symbols[i + 1])\n",
    "                    pairs[pair] = pairs.get(pair, 0) + freq\n",
    "\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            new_symbol = \"\".join(best_pair)\n",
    "            self.vocab[new_symbol] = len(self.vocab)\n",
    "            self.merges[best_pair] = new_symbol\n",
    "\n",
    "            new_word_freqs = {}\n",
    "            for word, freq in word_freqs.items():\n",
    "                symbols = list(word)\n",
    "                i = 0\n",
    "                while i < len(symbols) - 1:\n",
    "                    pair = (symbols[i], symbols[i + 1])\n",
    "                    if pair == best_pair:\n",
    "                        symbols[i] = new_symbol\n",
    "                        del symbols[i + 1]\n",
    "                    i += 1\n",
    "                new_word_freqs[\"\".join(symbols)] = freq\n",
    "            word_freqs = new_word_freqs\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode text into token IDs.\n",
    "        \"\"\"\n",
    "        symbols = list(text)\n",
    "        while len(symbols) > 1:\n",
    "            pairs = [(symbols[i], symbols[i + 1]) for i in range(len(symbols) - 1)]\n",
    "            best_pair = None\n",
    "            for pair in pairs:\n",
    "                if pair in self.merges:\n",
    "                    best_pair = pair\n",
    "                    break\n",
    "            if best_pair is None:\n",
    "                break\n",
    "            new_symbol = self.merges[best_pair]\n",
    "            new_symbols = []\n",
    "            i = 0\n",
    "            while i < len(symbols):\n",
    "                if i < len(symbols) - 1 and (symbols[i], symbols[i + 1]) == best_pair:\n",
    "                    new_symbols.append(new_symbol)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "            symbols = new_symbols\n",
    "\n",
    "        return [self.special_tokens[\"[SOS]\"]] + [self.vocab[s] for s in symbols if s in self.vocab] + [self.special_tokens[\"[EOS]\"]]\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode token IDs back into text.\n",
    "        \"\"\"\n",
    "        inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        return \"\".join([inv_vocab[token] for token in token_ids if token not in self.special_tokens.values()])\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        \"\"\"\n",
    "        Get the ID of a special token.\n",
    "        \"\"\"\n",
    "        return self.special_tokens.get(token, self.vocab.get(token, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "\n",
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, tokenizer_src, tokenizer_tgt, src_lang=\"fr\", tgt_lang=\"en\", seq_len=50):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor([0], dtype=torch.int64)  # Define manually since custom tokenizer has no ID mapping\n",
    "        self.eos_token = torch.tensor([1], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([2], dtype=torch.int64)\n",
    "\n",
    "        # Load dataset\n",
    "        self.dataset = []\n",
    "        with open(dataset_path, newline='', encoding='utf-8') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                self.dataset.append({\n",
    "                    \"src_text\": row[src_lang],\n",
    "                    \"tgt_text\": row[tgt_lang]\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.dataset[idx]['src_text']\n",
    "        tgt_text = self.dataset[idx]['tgt_text']\n",
    "\n",
    "        # Tokenize using custom BPE tokenizer\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text)\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text)\n",
    "\n",
    "        # Ensure sequence length constraints\n",
    "        enc_padding = self.seq_len - len(enc_input_tokens) - 2  # -2 for SOS and EOS\n",
    "        dec_padding = self.seq_len - len(dec_input_tokens) - 1  # -1 for SOS\n",
    "\n",
    "        if enc_padding < 0 or dec_padding < 0:\n",
    "            raise ValueError(\"Sentence is too long for the specified sequence length.\")\n",
    "\n",
    "        # Prepare inputs with SOS/EOS/PAD\n",
    "        encoder_input = torch.cat([\n",
    "            self.sos_token,\n",
    "            torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "            self.eos_token,\n",
    "            torch.tensor([self.pad_token] * enc_padding, dtype=torch.int64),\n",
    "        ])\n",
    "\n",
    "        decoder_input = torch.cat([\n",
    "            self.sos_token,\n",
    "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "            torch.tensor([self.pad_token] * dec_padding, dtype=torch.int64),\n",
    "        ])\n",
    "\n",
    "        label = torch.cat([\n",
    "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "            self.eos_token,\n",
    "            torch.tensor([self.pad_token] * dec_padding, dtype=torch.int64),\n",
    "        ])\n",
    "\n",
    "        # Ensure shapes are correct\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,\n",
    "            \"decoder_input\": decoder_input,\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
    "            \"label\": label,\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "\n",
    "def causal_mask(size):\n",
    "    \"\"\"\n",
    "    Create a triangular mask for the decoder. \n",
    "    This ensures that the model only attends to previous tokens and cannot \"see the future\" during training.\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": 'opus_books',\n",
    "        \"lang_src\": \"fr\",\n",
    "        \"lang_tgt\": \"en\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }\n",
    "\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)\n",
    "\n",
    "# Find the latest weights file in the weights folder\n",
    "def latest_weights_file_path(config):\n",
    "    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[UNK] → Unknown token = word or subword is not in the tokenizer's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "\n",
    "    if not tokenizer_path.exists():\n",
    "        print(f\"Training new tokenizer for {lang}...\")\n",
    "        \n",
    "        # Initialize tokenizer with special tokens\n",
    "        tokenizer = BPETokenizer(num_merges=100)\n",
    "        tokenizer.special_tokens.update({\n",
    "            \"[UNK]\": 3\n",
    "        })\n",
    "\n",
    "        # Train the tokenizer\n",
    "        tokenizer.train(get_all_sentences(ds, lang))\n",
    "\n",
    "        # Save tokenizer as JSON\n",
    "        with open(tokenizer_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"vocab\": tokenizer.vocab,\n",
    "                \"merges\": tokenizer.merges,\n",
    "                \"special_tokens\": tokenizer.special_tokens\n",
    "            }, f)\n",
    "    else:\n",
    "        print(f\"Loading tokenizer from {tokenizer_path}...\")\n",
    "        \n",
    "        # Load tokenizer from file\n",
    "        with open(tokenizer_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        tokenizer = BPETokenizer()\n",
    "        tokenizer.vocab = data[\"vocab\"]\n",
    "        tokenizer.merges = data[\"merges\"]\n",
    "        tokenizer.special_tokens = data[\"special_tokens\"]\n",
    "\n",
    "    return tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
