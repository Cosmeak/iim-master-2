{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Transformer Model\n",
    "\n",
    "The Transformer model is a groundbreaking architecture in AI, especially for Natural Language Processing (NLP) tasks like language translation, text generation, and more. It was introduced in 2017 by Vaswani et al. in a paper titled \"Attention Is All You Need\". This model was designed to handle sequential data (like sentences) in a much more efficient way than previous models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks.\n",
    "\n",
    "## Key Components of the Transformer\n",
    "\n",
    "The transformer architecture has a few critical building blocks that set it apart:\n",
    "\n",
    "### a. Self-Attention Mechanism\n",
    "\n",
    "What is Attention? The self-attention mechanism helps the model figure out which words in a sentence are most important relative to other words. For example, in the sentence \"The cat sat on the mat.\", the model needs to understand that \"sat\" relates closely to both \"cat\" and \"mat.\"\n",
    "\n",
    "Why Self-Attention? Traditional RNNs processed words one-by-one (sequentially), which makes it harder to understand long-range dependencies. With self-attention, the model can look at all words in the sentence at the same time and weigh their relationships.\n",
    "\n",
    "How Does It Work? Self-attention takes in three components for each word:\n",
    "- Query (Q)\n",
    "- Key (K)\n",
    "- Value (V)\n",
    "\n",
    "Each word in a sentence will compute attention scores by comparing its Query to all other words' Keys. These attention scores tell us how much focus should be placed on each word when generating a word’s output. The result is weighted by the Value of the word.\n",
    "\n",
    "### b. Multi-Head Attention\n",
    "\n",
    "Instead of having just one attention mechanism, the transformer model uses multiple attention heads. Each head learns a different aspect of the relationships between words in the sentence.\n",
    "\n",
    "Why Multiple Heads? With multiple heads, the model can capture different patterns in the relationships. For instance, one head may focus on subject-verb relationships, while another might focus on noun-adjective pairings.\n",
    "\n",
    "### c. Positional Encoding\n",
    "\n",
    "Transformers process input data all at once (in parallel), so they don't inherently know the order of the words in a sentence.\n",
    "\n",
    "To fix this, positional encoding is added to the input embeddings to represent the position of each word in the sequence. This encoding ensures the model understands the order of the words, which is crucial for meaning.\n",
    "\n",
    "### d. Feed-Forward Neural Networks\n",
    "\n",
    "After the attention layers, the transformer uses feed-forward layers (fully connected layers) to process the output from the attention mechanism and transform the data further.\n",
    "\n",
    "These layers help learn complex transformations that improve the overall model’s performance.\n",
    "\n",
    "## Why Transformers Are So Effective\n",
    "### a. Parallelization\n",
    "\n",
    "One of the key advantages of transformers is that they process all the words in a sentence simultaneously (in parallel). Unlike RNNs or LSTMs, which process one word at a time, transformers don’t have to wait for the previous word’s processing to be completed before moving on to the next one.\n",
    "This makes transformers much faster to train and scalable to large datasets.\n",
    "\n",
    "### b. Long-Range Dependencies\n",
    "\n",
    "Transformers are better at capturing long-range dependencies in text. For instance, in a sentence like “The man who was standing at the door left the house,” the model can connect “man” with “left” even though there are words in between. Traditional models like RNNs struggle with such dependencies, especially when sentences are long.\n",
    "\n",
    "### c. Flexibility in Tasks\n",
    "\n",
    "Transformers can handle a variety of tasks like translation, summarization, question-answering, and text generation.\n",
    "This flexibility is because the encoder-decoder architecture can be adapted. For instance, the decoder part can be used for generating text, while the encoder can be used for understanding and classifying text.\n",
    "\n",
    "## How Transformers are Used in Real Life\n",
    "\n",
    "Transformers have revolutionized the field of AI, and they're the backbone of many cutting-edge models:\n",
    "\n",
    "- GPT (Generative Pre-trained Transformer): This is the model behind AI tools like ChatGPT. It uses a transformer decoder architecture to generate human-like text.\n",
    "- BERT (Bidirectional Encoder Representations from Transformers): BERT uses the transformer encoder to understand context and perform tasks like sentiment analysis, named entity recognition, and more.\n",
    "- T5 (Text-to-Text Transfer Transformer): T5 treats every NLP problem as a text-to-text problem. For example, translating text from one language to another is treated as \"Translate English to French: [text].\"\n",
    "\n",
    "## Make my own Transformer Model from scratch\n",
    "\n",
    "### Transformer Architecture: Encoder-Decoder Structure\n",
    "\n",
    "The transformer model is typically divided into two parts:\n",
    "\n",
    "- Encoder: This part processes the input data (e.g., a sentence in one language).\n",
    "- Decoder: This part generates the output data (e.g., the translation of that sentence).\n",
    "\n",
    "Each of these parts is made up of layers that are stacked on top of each other:\n",
    "- Encoder Layers: Each encoder layer consists of two main components:\n",
    "    - Multi-head self-attention\n",
    "    - Feed-forward neural network\n",
    "- Decoder Layers: The decoder is similar, but with an additional layer of masked multi-head self-attention (which ensures that the decoder cannot \"cheat\" by looking ahead at future words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_folder = 'model/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq = seq\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create a matrix of shape (seq, d_model)\n",
    "        pe = torch.zeros(seq, d_model)\n",
    "        \n",
    "        # Create a vector of shape (seq)\n",
    "        position = torch.arange(0, seq, dtype=torch.float).unsqueeze(1) # (seq, 1)\n",
    "        \n",
    "        # Create a vector of shape (d_model)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)\n",
    "        \n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))\n",
    "        \n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))\n",
    "        \n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, seq, d_model)\n",
    "        \n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq, d_model)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, eps:float=10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features)) # alpha (multiplicative) is a learnable parameter\n",
    "        self.bias = nn.Parameter(torch.zeros(features)) # bias (additive) is a learnable parameter\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq, hidden_size)\n",
    "        # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim = -1, keepdim = True) # (batch, seq, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim = -1, keepdim = True) # (batch, seq, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq, d_model) --> (batch, seq, d_ff) --> (batch, seq, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Embedding vector size\n",
    "        self.h = h # Number of heads\n",
    "        # Make sure d_model is divisible by h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "\n",
    "        self.d_k = d_model // h # Dimension of vector seen by each head\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Just apply the formula from the paper\n",
    "        # (batch, h, seq, d_k) --> (batch, h, seq, seq)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            # Write a very low value (indicating -inf) to the positions where mask == 0\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq, seq) # Apply softmax\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # (batch, h, seq, seq) --> (batch, h, seq, d_k)\n",
    "        # return attention scores which can be used for visualization\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q) # (batch, seq, d_model) --> (batch, seq, d_model)\n",
    "        key = self.w_k(k) # (batch, seq, d_model) --> (batch, seq, d_model)\n",
    "        value = self.w_v(v) # (batch, seq, d_model) --> (batch, seq, d_model)\n",
    "\n",
    "        # (batch, seq, d_model) --> (batch, seq, h, d_k) --> (batch, h, seq, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Calculate attention\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # Combine all the heads together\n",
    "        # (batch, h, seq, d_k) --> (batch, seq, h, d_k) --> (batch, seq, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # Multiply by Wo\n",
    "        # (batch, seq, d_model) --> (batch, seq, d_model)  \n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "    \n",
    "        def forward(self, x, sublayer):\n",
    "            return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        features: int, \n",
    "        self_attention_block: MultiHeadAttentionBlock, \n",
    "        cross_attention_block: MultiHeadAttentionBlock, \n",
    "        feed_forward_block: FeedForwardBlock, \n",
    "        dropout: float\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "        # (batch, seq, d_model) --> (batch, seq, vocab_size)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder: Encoder, \n",
    "        decoder: Decoder, \n",
    "        src_embed: InputEmbeddings, \n",
    "        tgt_embed: InputEmbeddings, \n",
    "        src_pos: PositionalEncoding, \n",
    "        tgt_pos: PositionalEncoding, \n",
    "        projection_layer: ProjectionLayer\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # (batch, seq, d_model)\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, encoder_output: torch.Tensor, src_mask: torch.Tensor, tgt: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        # (batch, seq, d_model)\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        # (batch, seq, vocab_size)\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def build_transformer(\n",
    "    src_vocab_size: int, \n",
    "    tgt_vocab_size: int, \n",
    "    src_seq: int, \n",
    "    tgt_seq: int, \n",
    "    d_model: int=512, \n",
    "    N: int=6, \n",
    "    h: int=8, \n",
    "    dropout: float=0.1, \n",
    "    d_ff: int=2048\n",
    ") -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq, dropout)\n",
    "    \n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the decoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "    \n",
    "    # Create the encoder and decoder\n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "    \n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "    \n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Tokenizer?\n",
    "\n",
    "A tokenizer is a tool or algorithm used to break down text into smaller units called tokens. These tokens can be words, subwords, or even characters, depending on the type of tokenizer used.\n",
    "\n",
    "#### a. Why Tokenize?\n",
    "\n",
    "Text Processing: Computers cannot directly understand raw text. Tokenization transforms text into smaller, more manageable parts (tokens) that AI models can process.\n",
    "\n",
    "Building Blocks: Tokens are the building blocks for further processing, like embedding (turning tokens into numerical vectors) or training models.\n",
    "\n",
    "#### b. Types of Tokenization\n",
    "\n",
    "Word-level Tokenization:\n",
    "- Breaks the text into individual words.\n",
    "- Example: \"I love pizza!\" → [\"I\", \"love\", \"pizza\", \"!\"]\n",
    "- This is simple but doesn't handle variations well (e.g., \"running\" and \"ran\" would be treated as separate words).\n",
    "\n",
    "Character-level Tokenization:\n",
    "- Breaks the text into individual characters.\n",
    "- Example: \"love\" → ['l', 'o', 'v', 'e']\n",
    "- This is useful for languages with complex word formations but results in more tokens.\n",
    "\n",
    "Subword Tokenization: (***thats what we will use here***)\n",
    "- Breaks words into smaller meaningful units, like prefixes or suffixes.\n",
    "- Example: \"unhappiness\" → [\"un\", \"happiness\"]\n",
    "- This is often used in Byte Pair Encoding (BPE) or WordPiece, and helps handle unknown or rare words.\n",
    "\n",
    "<br>\n",
    "\n",
    "> **What is BPE Tokenizer?**\n",
    "> \n",
    "> Byte Pair Encoding (BPE) is a subword tokenization method that iteratively merges the most frequent pairs of bytes or characters in a corpus of text. It is commonly used to break down words into smaller, more frequent subword units, making it easier for models to handle rare or unseen words.\n",
    "> \n",
    "> **Usage**\n",
    "> \n",
    "> Handling Rare Words: BPE can break down rare or unknown words into subword units that the model has seen during training. This helps avoid the \"out-of-vocabulary\" problem.\n",
    ">\n",
    "> Balance: It balances between character-level and word-level tokenization, capturing frequent subword patterns without creating too many tokens (like character-level tokenization).\n",
    ">\n",
    "> Compression: BPE helps compress the vocabulary size and reduces memory usage for handling words in NLP tasks.\n",
    "\n",
    "#### c. Why is Tokenization Important?\n",
    "\n",
    "Model Input: AI models can only work with numbers. Tokenization converts text into tokens that can then be transformed into numerical vectors.\n",
    "\n",
    "Handling Vocabulary: Tokenizers help manage vocabulary size, especially when dealing with complex or large text corpora.\n",
    "\n",
    "Preprocessing: It's an essential part of preprocessing before passing text to models like BERT, GPT, etc.\n",
    "\n",
    "--- \n",
    "\n",
    "> **Special tokens used below:**\n",
    "> \n",
    "> [**SOS**] → Signals the start of a sentence (important for decoding).\n",
    "> \n",
    "> [**EOS**] → Indicates the end of a sentence (used in labels).\n",
    "> \n",
    "> [**PAD**] → Pads shorter sentences to match seq_len (ignored during computation).\n",
    ">\n",
    "> [**UNK**] → Unknown token = word or subword is not in the tokenizer's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, num_merges=100):\n",
    "        self.num_merges = num_merges\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "\n",
    "        # Special tokens\n",
    "        self.special_tokens = {\n",
    "            \"[SOS]\": 0,\n",
    "            \"[EOS]\": 1,\n",
    "            \"[PAD]\": 2\n",
    "        }\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer on a given text corpus.\n",
    "        \"\"\"\n",
    "        word_freqs = {}\n",
    "        for sentence in corpus:\n",
    "            if sentence == None: \n",
    "                continue\n",
    "            words = sentence.split()\n",
    "            for word in words:\n",
    "                word_freqs[word] = word_freqs.get(word, 0) + 1\n",
    "\n",
    "        # Initialize vocabulary with words\n",
    "        self.vocab = {word: idx + 3 for idx, word in enumerate(word_freqs.keys())}\n",
    "        self.vocab.update(self.special_tokens)\n",
    "\n",
    "        # Merge operations\n",
    "        for _ in range(self.num_merges):\n",
    "            pairs = {}\n",
    "            for word, freq in word_freqs.items():\n",
    "                symbols = word.split()\n",
    "                for i in range(len(symbols) - 1):\n",
    "                    pair = (symbols[i], symbols[i + 1])\n",
    "                    pairs[pair] = pairs.get(pair, 0) + freq\n",
    "\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            new_symbol = \" \".join(best_pair)\n",
    "            self.vocab[new_symbol] = len(self.vocab)\n",
    "            self.merges[best_pair] = new_symbol\n",
    "\n",
    "            new_word_freqs = {}\n",
    "            for word, freq in word_freqs.items():\n",
    "                symbols = word.split()\n",
    "                i = 0\n",
    "                while i < len(symbols) - 1:\n",
    "                    pair = (symbols[i], symbols[i + 1])\n",
    "                    if pair == best_pair:\n",
    "                        symbols[i] = new_symbol\n",
    "                        del symbols[i + 1]\n",
    "                    i += 1\n",
    "                new_word_freqs[\" \".join(symbols)] = freq\n",
    "            word_freqs = new_word_freqs\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode text into token IDs.\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        while len(words) > 1:\n",
    "            pairs = [(words[i], words[i + 1]) for i in range(len(words) - 1)]\n",
    "            best_pair = None\n",
    "            for pair in pairs:\n",
    "                if pair in self.merges:\n",
    "                    best_pair = pair\n",
    "                    break\n",
    "            if best_pair is None:\n",
    "                break\n",
    "            new_symbol = self.merges[best_pair]\n",
    "            new_words = []\n",
    "            i = 0\n",
    "            while i < len(words):\n",
    "                if i < len(words) - 1 and (words[i], words[i + 1]) == best_pair:\n",
    "                    new_words.append(new_symbol)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_words.append(words[i])\n",
    "                    i += 1\n",
    "            words = new_words\n",
    "\n",
    "        return [self.special_tokens[\"[SOS]\"]] + [self.vocab[w] for w in words if w in self.vocab] + [self.special_tokens[\"[EOS]\"]]\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode token IDs back into text.\n",
    "        \"\"\"\n",
    "        inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        return \" \".join([inv_vocab[token] for token in token_ids if token not in self.special_tokens.values()])\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        \"\"\"\n",
    "        Get the ID of a special token.\n",
    "        \"\"\"\n",
    "        return self.special_tokens.get(token, self.vocab.get(token, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import csv\n",
    "\n",
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_raw, tokenizer_src, tokenizer_tgt, src_lang=\"fr\", tgt_lang=\"en\", seq_len=50):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        self.sos_token = torch.tensor([0], dtype=torch.int64)  # Define manually since custom tokenizer has no ID mapping\n",
    "        self.eos_token = torch.tensor([1], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([2], dtype=torch.int64)\n",
    "\n",
    "        # Load dataset\n",
    "        self.dataset = []\n",
    "        for row in dataset_raw: \n",
    "            self.dataset.append({\n",
    "                \"src_text\": row[src_lang],\n",
    "                \"tgt_text\": row[tgt_lang]\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.dataset[idx]['src_text']\n",
    "        tgt_text = self.dataset[idx]['tgt_text']\n",
    "\n",
    "        # Tokenize using custom BPE tokenizer\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text)\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text)\n",
    "\n",
    "        # Ensure sequence length constraints\n",
    "        enc_padding = self.seq_len - len(enc_input_tokens) - 2  # -2 for SOS and EOS\n",
    "        dec_padding = self.seq_len - len(dec_input_tokens) - 1  # -1 for SOS\n",
    "\n",
    "        if enc_padding < 0 or dec_padding < 0:\n",
    "            raise ValueError(\"Sentence is too long for the specified sequence length.\")\n",
    "\n",
    "        # Prepare inputs with SOS/EOS/PAD\n",
    "        encoder_input = torch.cat([\n",
    "            self.sos_token,\n",
    "            torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "            self.eos_token,\n",
    "            torch.tensor([self.pad_token] * enc_padding, dtype=torch.int64),\n",
    "        ])\n",
    "\n",
    "        decoder_input = torch.cat([\n",
    "            self.sos_token,\n",
    "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "            torch.tensor([self.pad_token] * dec_padding, dtype=torch.int64),\n",
    "        ])\n",
    "\n",
    "        label = torch.cat([\n",
    "            torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "            self.eos_token,\n",
    "            torch.tensor([self.pad_token] * dec_padding, dtype=torch.int64),\n",
    "        ])\n",
    "\n",
    "        # Ensure shapes are correct\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,\n",
    "            \"decoder_input\": decoder_input,\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
    "            \"label\": label,\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }\n",
    "\n",
    "def causal_mask(size):\n",
    "    \"\"\"\n",
    "    Create a triangular mask for the decoder. \n",
    "    This ensures that the model only attends to previous tokens and cannot \"see the future\" during training.\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": 'FrancophonIA/french-to-english',\n",
    "        \"lang_src\": \"fr\",\n",
    "        \"lang_tgt\": \"en\",\n",
    "        \"model_folder\": \"runs\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"model/data/tokenizers/tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }\n",
    "\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = f\"{data_folder}/{config['model_folder']}/{config['datasource']}\"\n",
    "    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n",
    "    return str(Path('.') / model_folder / model_filename)\n",
    "\n",
    "# Find the latest weights file in the weights folder\n",
    "def latest_weights_file_path(config):\n",
    "    model_folder = f\"{data_folder}/{config['model_folder']}/{config['datasource']}_{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "\n",
    "    if not tokenizer_path.exists():\n",
    "        print(f\"Training new tokenizer for {lang}...\")\n",
    "        \n",
    "        # Initialize tokenizer with special tokens\n",
    "        tokenizer = BPETokenizer(num_merges=100)\n",
    "        tokenizer.special_tokens.update({\n",
    "            \"[UNK]\": 3\n",
    "        })\n",
    "\n",
    "        # Train the tokenizer\n",
    "        tokenizer.train(get_all_sentences(ds, lang))\n",
    "\n",
    "        # Save tokenizer as JSON\n",
    "        with open(tokenizer_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"vocab\": tokenizer.vocab,\n",
    "                \"merges\": tokenizer.merges,\n",
    "                \"special_tokens\": tokenizer.special_tokens\n",
    "            }, f)\n",
    "    else:\n",
    "        print(f\"Loading tokenizer from {tokenizer_path}...\")\n",
    "        \n",
    "        # Load tokenizer from file\n",
    "        with open(tokenizer_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        tokenizer = BPETokenizer()\n",
    "        tokenizer.vocab = data[\"vocab\"]\n",
    "        tokenizer.merges = data[\"merges\"]\n",
    "        tokenizer.special_tokens = data[\"special_tokens\"]\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_ds(config):\n",
    "    # It only has the train split, so we divide it overselves\n",
    "    ds_raw = load_dataset(f\"{config['datasource']}\", \"default\", split='train')\n",
    "\n",
    "    # Build tokenizers\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "\n",
    "    # Keep 90% for training, 10% for validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    # train_ds = train_ds_raw.set_format(type=\"torch\")\n",
    "    # val_ds = val_ds_raw.set_format(type=\"torch\")\n",
    "    train_ds = TranslationDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq'])\n",
    "    val_ds = TranslationDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq'])\n",
    "\n",
    "    # Find the maximum length of each sentence in the source and target sentence\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item[config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item[config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq\"], config['seq'], d_model=config['d_model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item[lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Define the device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "    if (device == 'cuda'):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "    elif (device == 'mps'):\n",
    "        print(f\"Device name: <mps>\")\n",
    "    else:\n",
    "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
    "        print(\"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n",
    "        print(\"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{data_folder}/{config['model_folder']}/{config['datasource']}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "\n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "    if model_filename:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq, seq)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Device name: <mps>\n",
      "Loading tokenizer from model/data/tokenizers/tokenizer_fr.json...\n",
      "Loading tokenizer from model/data/tokenizers/tokenizer_en.json...\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "train_model(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
